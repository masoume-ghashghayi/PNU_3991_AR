\documentclass{book} 
\usepackage[top=3cm,right=3.5cm,bottom=3cm,left=3cm]{geometry}
\usepackage{enumitem}
\renewcommand{\baselinestretch}{1.7}
\parindent=5pt
\begin{document}
page$142$\\\\\par
containing information about the user (see David VWnalen's informative Frequently Asked Questions about cookies for a more detailed discussion of the technical, privacy, and security issues related to cookies at http://uww.cookiecentral.com/faq/). Unfortunately, it can be very challenging to trace all the activities of different users. For example, users may leave the Web environment to search external sites, take breaks of indeterminate lengths during the session, or abruptly terminate the session for a reason that is not apparent to the researcher but is easily explained by the user. In addition, many users set their browsers to extract contents of repeated pages from the browser's internal cache, rather than repeating calls to the server, thus introducing discontinuities in the log, As we can see from these examples, analving Web logs presents a number of challenges to the e-researcher. Abrams and Williams (1996) describe four different means of accessing Web logs, including a client-side monitor that users install on their own machines to gather data on network usage. They also note how surveys and interviews can be used in conjunction with Web log analysis to further inform and explain some of the anomalies that arise when deducing user behavior solely from the Web logs. Forest Stroud also maintains a review of the commercial Web analysis at http://cws.internet.com/reviews/analysis-reviews .html, and Web analysis software packages are regularly reviewed by ZDNet at www.zdnet.com.\par Most of the sites studied bv educationally orientated e-researchers are relatively small and frequented by a limited number of users. However, site mining information can also be gathered on large sites producing very interesting results. For example, Spink and Xu (2000) analyzed the results of user search behavior using the large, global Excite search engine that responds to over 30 million requests per day! They report summary data on the 30 billion requests to the Excite search engine between 1996 and 1999 and determined that only one in eighteen search requests used Boolean operators to refine the search. Of these searches, fully 50 percent made mistakes in the use of the terms according to Excite operation rules.\\
\textbf{WHO IS REALLY VISITING MY SITE? PROBLEMS OF PROXIES AND ANONYMOUS USERS}\par  Problems are emerging with respect to the increasing number of users who enter sites from behind a type of personal or corporate firewall known as n proxy sewer. A proxy server works by intercepting requests from an individual's machine, perhaps filtering, allowing, or disallowing certain sites, and then sending the request to the targeted Web site. The Web site returns the requested information to the proxy server, rather than directly to the end user. Thus, the Web Server cannot determine who the real user was an issue that is complicated if many users are accessing the same proxy server, such as on a large University campus. In addition, some users are willing to pay (currently about \$5.00 per month) for privacy services, such as Anonymizer (http://uww.anonymizer.com), that use a proprietary proxy server to filter information about individual use while accessing resources on the Net. These servers also sell\newpage
page$143$\\\\\par
anonymous and encrypted email and Web publishing services. It is unlikely that users willing to pay for such premium security will be frequent participants in e-research projects, but you may have occasion to run into such a disguised user. Our advice, if this happens, is to delete the subject and any data gathered from your study, unless you are after a very broad analysis of user behavior with no regard to individual or unique identities.\par 
Like almost all e-research, usage analysis is most effective and useful when it is tied theoretically to previous studies and explanations of user behavior. Choo, Detlor, and Turnbull (1998), for example, describe how theoretical models of user search behavior are verified and elaborated on based on analysis Of client-side user logs.\\
\textbf{USE OF THE WEB FOR OBSERVATION OF NET-BASED ACTIVITIES}\par 
 A common activity integrated into the preserviee training Ofstudent teachers and other preprofessionals is to observe and document the activities Of expert professionals as they interact with real clients, in real contexts. This practice has considerable value in that it lets the student (or researcher) observe real professional activity and provides a wide field of vision allowing students to focus and attend to particular details. In this section, we discuss ways in which the Web can be used as the eyes and ears of the e-researcher, providing observation and recording capacity anytime/anywhere.\par 
The Net provides a much less expensive way to augment or replace direct human observation through the use of remote sensing devices including Web cameras (WebCams) and other forms of remote sensing devices. For example, a recent search of the AllCam (http://www.allcam.com) Web mega site provides links to 197 live Web cameras classified under the subject of education. Of these sites, fifteen were broadcasting live feeds from classrooms or lecture halls. These numbers show the popularity and increasing ubiquity of WebCams, but we are not suggesting that watching these public access cameras for twenty-four hours a day will do much for the e-researcher or for educational research other than give the researcher a mega-dose of eye strain! Rather, we are suggesting that WebCarn techniques can be applied to create new forms ofeducational research.\par 
Web-based observation builds on the data-gathering techniques developed for direct Observation during professional practice. Direct observation offers the researcher a real-time look at behaviors that are Only inferred from paper-and-pen- based surveys, personal recollections ofevents, and other indirect or time-shifted data- gathering techniques. It also allows the e-researcher to focus in detail on sorne particular aspect of the scene, which may not even be noticed by participants.\par 
Steps in planning and executing a WebCam-based observation are similar to those involved in any study that uses direct observation of behavior.\\ 
\begin{itemize}
\item Determine a hypothesis, problem area, or behavior of interest, usually beginning with a theoretical basis for this interest.
\item Locate a site for the observation.
\end{itemize}
\newpage
page$144$\\\\\par
\begin{itemize}
\item Develop a coding scheme and train additional observers to reliably identify instances of the behavior under study.
\item Choose a scheme to systematically define or count the extent of the behavior. This is sometimes difficult as observers can become confused when trying to differentiate between the frequency and the duration of any behavior. Typical schemes include time sampling, running record, and event sampling.
\item Quantify and tabulate results. 
\item Investigate and calculate relationships between observed behavior and other theoretically important variables.
\item Report and disseminate results.
\end{itemize}\par
There are a number of technical and procedural differences that add complexity to the research when remote observations via the Net are involved in the data collection. These considerations relate to the type and detail of photographs needed, the acquisitions of appropriate hardware, software, and Internet connection, and the complexity of gaining approval and informed consent from participants.\par 
The first decision facing e-researchers who are contemplating Web camera observations is to decide if still frame or video is needed. WebCams can be automated to take still frame pictures on a scheduled basis say every ten seconds and transfer these images to a storage device or a Web server. Alternatively, it is possible to gather full- or half-screen video images directly, however, this process requires much higher speed connections to the Net to both transmit and receive the signals. Television cameras capture video at thirty frames per second. Capturing, digitizing, and transmitting a typical 100-KB image thirty times a second creates a load of 3 ,000 KB a second. This is nearly 150 times the bandwidth supplied from a typical 56 KB modem, however, with higher speed Local Area Network connections, reduction of the size of the image and the number of images captured per second, and continuing improvements in compression techniques, it is possible to capture and stream live video. Deciding if the video is satisfactory for the objectives of the e-researcher is a challenge that is perhaps only resolved by trial and error.\par 
Next, e-researchers must deal with the complexities related to the necessary hardware. The first task is acquisition and installation of the necessary hardware usually a camera, mounted in the correct location and connected to a computer. There are a variety of very useful and detailed guides on the Net to aid in the selection of appropriate hardware. The AllCam Web site mentioned earlier provides tutorials of its own and links to additional tutorials. We were especially impressed with the informal but informative WebCam Cookbook site maintained by Sam Churchill at http://www.teleport.com /\%7Esamc /bike/#Make . The WebCam can be connected by Cable to a desktop or n portable computer. There are also a number of commercial firms who manufacturer custom WebCams with a dedicated computer built into the camera. Some systems also allow the viewer to remotely control the viewing angle and focus Of the WebCam allowing for zooming in and closer view of behaviors of particular interest.\par 
The e-researcher must then acquire and install the appropriate software on the WebCam system. Naturally there is different software for different operating systems,
\end{document}